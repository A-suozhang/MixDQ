: <class 'qdiff.quant_model.QuantModel'>
model: <class 'ldm.modules.diffusionmodules.openaimodel.UNetModel'>
model.time_embed: <class 'torch.nn.modules.container.Sequential'>
model.time_embed.0: <class 'qdiff.quant_layer.QuantModule'>
model.time_embed.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.time_embed.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.time_embed.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.time_embed.1: <class 'torch.nn.modules.activation.SiLU'>
model.time_embed.2: <class 'qdiff.quant_layer.QuantModule'>
model.time_embed.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.time_embed.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.time_embed.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>


model.input_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.input_blocks.0: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.input_blocks.0.0: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.0.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.0.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.0.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>

model.input_blocks.1: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.input_blocks.1.0: <class 'qdiff.quant_block.QuantResBlock'>
model.input_blocks.1.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.1.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.1.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.1.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.input_blocks.1.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.1.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.1.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.1.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.1.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.1.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.1.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.0.skip_connection: <class 'torch.nn.modules.linear.Identity'>
model.input_blocks.1.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.input_blocks.1.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.input_blocks.1.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.input_blocks.1.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.input_blocks.1.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.input_blocks.1.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.1.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.input_blocks.1.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.1.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.1.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.input_blocks.1.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.1.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.1.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.1.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.1.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.1.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.1.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>

model.input_blocks.2: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.input_blocks.2.0: <class 'qdiff.quant_block.QuantResBlock'>
model.input_blocks.2.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.2.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.2.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.2.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.input_blocks.2.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.2.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.2.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.2.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.2.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.2.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.2.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.0.skip_connection: <class 'torch.nn.modules.linear.Identity'>
model.input_blocks.2.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.input_blocks.2.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.input_blocks.2.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.input_blocks.2.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.input_blocks.2.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.input_blocks.2.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.2.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.input_blocks.2.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.2.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.2.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.input_blocks.2.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.2.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.2.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.2.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.2.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.2.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.2.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>

model.input_blocks.3: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.input_blocks.3.0: <class 'ldm.modules.diffusionmodules.openaimodel.Downsample'>
model.input_blocks.3.0.op: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.3.0.op.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.3.0.op.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.3.0.op.activation_function: <class 'qdiff.quant_layer.StraightThrough'>

model.input_blocks.4: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.input_blocks.4.0: <class 'qdiff.quant_block.QuantResBlock'>
model.input_blocks.4.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.4.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.4.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.4.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.input_blocks.4.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.4.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.4.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.4.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.4.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.4.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.4.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.input_blocks.4.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.input_blocks.4.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.input_blocks.4.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.input_blocks.4.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.input_blocks.4.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.input_blocks.4.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.4.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.4.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.input_blocks.4.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.4.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.4.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.4.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.4.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.4.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>

model.input_blocks.5: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.input_blocks.5.0: <class 'qdiff.quant_block.QuantResBlock'>
model.input_blocks.5.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.5.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.5.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.5.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.input_blocks.5.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.5.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.5.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.5.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.5.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.5.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.5.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.0.skip_connection: <class 'torch.nn.modules.linear.Identity'>
model.input_blocks.5.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.input_blocks.5.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.input_blocks.5.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.input_blocks.5.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.input_blocks.5.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.input_blocks.5.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.input_blocks.5.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.5.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.5.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.input_blocks.5.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.5.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.5.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.5.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.5.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.5.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>

model.input_blocks.6: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.input_blocks.6.0: <class 'ldm.modules.diffusionmodules.openaimodel.Downsample'>
model.input_blocks.6.0.op: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.6.0.op.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.6.0.op.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.6.0.op.activation_function: <class 'qdiff.quant_layer.StraightThrough'>

model.input_blocks.7: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.input_blocks.7.0: <class 'qdiff.quant_block.QuantResBlock'>
model.input_blocks.7.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.7.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.7.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.7.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.input_blocks.7.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.7.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.7.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.7.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.7.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.7.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.7.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.input_blocks.7.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.input_blocks.7.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.input_blocks.7.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.input_blocks.7.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.input_blocks.7.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.input_blocks.7.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.7.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.7.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.input_blocks.7.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.7.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.7.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.7.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.7.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.7.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>

model.input_blocks.8: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.input_blocks.8.0: <class 'qdiff.quant_block.QuantResBlock'>
model.input_blocks.8.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.8.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.8.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.8.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.input_blocks.8.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.8.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.8.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.8.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.8.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.8.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.8.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.0.skip_connection: <class 'torch.nn.modules.linear.Identity'>
model.input_blocks.8.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.input_blocks.8.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.input_blocks.8.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.input_blocks.8.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.input_blocks.8.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.input_blocks.8.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.input_blocks.8.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.8.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.8.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.input_blocks.8.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.8.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.8.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.input_blocks.8.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.8.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.8.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>

model.input_blocks.9: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.input_blocks.9.0: <class 'ldm.modules.diffusionmodules.openaimodel.Downsample'>
model.input_blocks.9.0.op: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.9.0.op.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.9.0.op.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.9.0.op.activation_function: <class 'qdiff.quant_layer.StraightThrough'>

model.input_blocks.10: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.input_blocks.10.0: <class 'qdiff.quant_block.QuantResBlock'>
model.input_blocks.10.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.10.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.10.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.10.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.10.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.10.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.10.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.10.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.10.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.10.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.input_blocks.10.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.10.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.10.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.10.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.10.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.10.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.10.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.10.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.10.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.10.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.10.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.10.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.10.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.10.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.10.0.skip_connection: <class 'torch.nn.modules.linear.Identity'>

model.input_blocks.11: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.input_blocks.11.0: <class 'qdiff.quant_block.QuantResBlock'>
model.input_blocks.11.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.11.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.11.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.11.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.11.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.11.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.11.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.11.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.11.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.11.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.input_blocks.11.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.11.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.11.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.11.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.11.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.11.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.11.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.input_blocks.11.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.input_blocks.11.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.input_blocks.11.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.input_blocks.11.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.input_blocks.11.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.11.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.input_blocks.11.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.input_blocks.11.0.skip_connection: <class 'torch.nn.modules.linear.Identity'>


model.middle_block: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.middle_block.0: <class 'qdiff.quant_block.QuantResBlock'>
model.middle_block.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.middle_block.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.middle_block.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.middle_block.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.middle_block.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.middle_block.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.middle_block.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.middle_block.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.middle_block.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.middle_block.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.middle_block.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.0.skip_connection: <class 'torch.nn.modules.linear.Identity'>
model.middle_block.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.middle_block.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.middle_block.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.middle_block.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.middle_block.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.middle_block.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.middle_block.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.middle_block.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.middle_block.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.middle_block.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.middle_block.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.middle_block.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.middle_block.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.middle_block.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.middle_block.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.middle_block.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.middle_block.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.middle_block.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.middle_block.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.middle_block.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.middle_block.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.middle_block.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.2: <class 'qdiff.quant_block.QuantResBlock'>
model.middle_block.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.2.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.middle_block.2.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.middle_block.2.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.middle_block.2.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.2.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.2.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.2.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.2.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.middle_block.2.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.middle_block.2.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.middle_block.2.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.2.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.2.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.2.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.2.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.middle_block.2.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.middle_block.2.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.middle_block.2.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.middle_block.2.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.middle_block.2.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.2.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.middle_block.2.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.middle_block.2.skip_connection: <class 'torch.nn.modules.linear.Identity'>


model.output_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.output_blocks.0: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.output_blocks.0.0: <class 'qdiff.quant_block.QuantResBlock'>
model.output_blocks.0.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.0.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.0.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.0.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.0.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.0.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.0.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.0.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.0.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.0.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.output_blocks.0.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.0.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.0.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.0.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.0.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.0.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.0.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.0.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.0.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.0.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.0.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.0.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.0.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.0.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.0.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.0.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.0.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.0.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.1: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.output_blocks.1.0: <class 'qdiff.quant_block.QuantResBlock'>
model.output_blocks.1.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.1.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.1.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.1.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.1.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.1.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.1.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.1.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.1.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.1.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.output_blocks.1.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.1.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.1.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.1.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.1.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.1.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.1.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.1.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.1.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.1.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.1.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.1.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.1.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.1.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.1.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.1.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.1.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.1.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.2: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.output_blocks.2.0: <class 'qdiff.quant_block.QuantResBlock'>
model.output_blocks.2.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.2.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.2.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.2.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.2.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.2.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.2.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.2.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.2.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.2.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.output_blocks.2.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.2.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.2.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.2.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.2.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.2.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.2.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.2.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.2.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.2.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.2.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.2.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.2.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.2.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.2.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.2.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.2.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.2.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.2.1: <class 'ldm.modules.diffusionmodules.openaimodel.Upsample'>
model.output_blocks.2.1.conv: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.2.1.conv.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.2.1.conv.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.2.1.conv.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.output_blocks.3.0: <class 'qdiff.quant_block.QuantResBlock'>
model.output_blocks.3.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.3.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.3.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.3.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.output_blocks.3.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.3.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.3.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.3.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.3.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.3.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.3.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.output_blocks.3.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.output_blocks.3.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.output_blocks.3.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.output_blocks.3.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.3.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.3.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.output_blocks.3.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.3.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.3.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.3.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.3.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.3.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.3.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.3.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.3.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.3.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.output_blocks.4.0: <class 'qdiff.quant_block.QuantResBlock'>
model.output_blocks.4.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.4.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.4.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.4.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.output_blocks.4.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.4.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.4.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.4.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.4.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.4.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.4.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.output_blocks.4.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.output_blocks.4.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.output_blocks.4.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.output_blocks.4.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.4.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.output_blocks.4.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.4.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.4.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.4.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.4.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.4.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.4.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.4.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.4.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.output_blocks.5.0: <class 'qdiff.quant_block.QuantResBlock'>
model.output_blocks.5.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.5.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.5.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.5.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.output_blocks.5.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.5.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.5.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.5.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.5.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.5.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.5.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.output_blocks.5.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.output_blocks.5.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.output_blocks.5.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.output_blocks.5.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.5.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.output_blocks.5.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.5.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.5.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.5.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.5.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.5.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.5.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.5.2: <class 'ldm.modules.diffusionmodules.openaimodel.Upsample'>
model.output_blocks.5.2.conv: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.5.2.conv.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.2.conv.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.5.2.conv.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.output_blocks.6.0: <class 'qdiff.quant_block.QuantResBlock'>
model.output_blocks.6.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.6.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.6.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.6.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.output_blocks.6.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.6.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.6.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.6.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.6.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.6.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.6.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.output_blocks.6.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.output_blocks.6.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.output_blocks.6.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.output_blocks.6.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.6.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.6.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.output_blocks.6.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.6.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.6.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.6.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.6.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.6.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.6.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.6.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.6.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.6.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.output_blocks.7.0: <class 'qdiff.quant_block.QuantResBlock'>
model.output_blocks.7.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.7.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.7.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.7.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.output_blocks.7.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.7.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.7.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.7.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.7.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.7.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.7.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.output_blocks.7.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.output_blocks.7.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.output_blocks.7.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.output_blocks.7.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.7.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.output_blocks.7.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.7.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.7.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.7.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.7.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.7.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.7.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.7.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.7.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.output_blocks.8.0: <class 'qdiff.quant_block.QuantResBlock'>
model.output_blocks.8.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.8.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.8.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.8.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.output_blocks.8.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.8.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.8.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.8.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.8.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.8.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.8.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.output_blocks.8.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.output_blocks.8.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.output_blocks.8.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.output_blocks.8.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.8.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.output_blocks.8.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.8.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.8.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.8.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.8.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.8.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.8.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.8.2: <class 'ldm.modules.diffusionmodules.openaimodel.Upsample'>
model.output_blocks.8.2.conv: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.8.2.conv.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.2.conv.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.8.2.conv.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.output_blocks.9.0: <class 'qdiff.quant_block.QuantResBlock'>
model.output_blocks.9.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.9.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.9.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.9.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.output_blocks.9.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.9.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.9.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.9.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.9.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.9.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.9.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.output_blocks.9.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.output_blocks.9.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.output_blocks.9.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.output_blocks.9.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.9.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.9.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.output_blocks.9.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.9.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.9.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.9.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.9.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.9.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.9.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.9.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.9.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.9.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.output_blocks.10.0: <class 'qdiff.quant_block.QuantResBlock'>
model.output_blocks.10.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.10.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.10.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.10.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.output_blocks.10.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.10.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.10.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.10.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.10.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.10.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.10.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.output_blocks.10.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.output_blocks.10.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.output_blocks.10.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.output_blocks.10.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.10.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.10.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.output_blocks.10.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.10.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.10.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.10.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.10.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.10.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.10.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.10.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.10.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.10.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11: <class 'ldm.modules.diffusionmodules.openaimodel.TimestepEmbedSequential'>
model.output_blocks.11.0: <class 'qdiff.quant_block.QuantResBlock'>
model.output_blocks.11.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.0.in_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.11.0.in_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.11.0.in_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.11.0.in_layers.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.0.in_layers.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.0.in_layers.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.0.in_layers.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.0.h_upd: <class 'torch.nn.modules.linear.Identity'>
model.output_blocks.11.0.emb_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.11.0.emb_layers.0: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.11.0.emb_layers.1: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.0.emb_layers.1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.0.emb_layers.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.0.emb_layers.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.0.out_layers: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.11.0.out_layers.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.output_blocks.11.0.out_layers.1: <class 'torch.nn.modules.activation.SiLU'>
model.output_blocks.11.0.out_layers.2: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.11.0.out_layers.3: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.0.out_layers.3.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.0.out_layers.3.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.0.out_layers.3.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.0.skip_connection: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.0.skip_connection.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.0.skip_connection.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.0.skip_connection.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.1: <class 'ldm.modules.attention.SpatialTransformer'>
model.output_blocks.11.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.output_blocks.11.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.output_blocks.11.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantBasicTransformerBlock'>
model.output_blocks.11.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.1.transformer_blocks.0.attn1: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.1.transformer_blocks.0.attn1.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.11.1.transformer_blocks.0.attn1.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.11.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.ff: <class 'ldm.modules.attention.FeedForward'>
model.output_blocks.11.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.11.1.transformer_blocks.0.ff.net.0: <class 'ldm.modules.attention.GEGLU'>
model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.11.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.1.transformer_blocks.0.attn2: <class 'ldm.modules.attention.CrossAttention'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.1.transformer_blocks.0.attn2.qk_matmul: <class 'ldm.modules.attention.CrossQKMatMul'>
model.output_blocks.11.1.transformer_blocks.0.attn2.smv_matmul: <class 'ldm.modules.attention.CrossSMVMatMul'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.Sequential'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.output_blocks.11.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.11.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.11.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.output_blocks.11.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.output_blocks.11.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.output_blocks.11.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>


model.out: <class 'torch.nn.modules.container.Sequential'>
model.out.0: <class 'ldm.modules.diffusionmodules.util.GroupNorm32'>
model.out.1: <class 'torch.nn.modules.activation.SiLU'>
model.out.2: <class 'qdiff.quant_layer.QuantModule'>
model.out.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.out.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.out.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
