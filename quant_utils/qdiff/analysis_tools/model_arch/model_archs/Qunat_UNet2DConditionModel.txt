: <class 'qdiff.quant_model.QuantModel'>
model: <class 'diffusers.models.unet_2d_condition.UNet2DConditionModel'>
model.conv_in: <class 'qdiff.quant_layer.QuantModule'>
model.conv_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.conv_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.conv_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.time_proj: <class 'diffusers.models.embeddings.Timesteps'>
model.time_embedding: <class 'diffusers.models.embeddings.TimestepEmbedding'>
model.time_embedding.linear_1: <class 'qdiff.quant_layer.QuantModule'>
model.time_embedding.linear_1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.time_embedding.linear_1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.time_embedding.linear_1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.time_embedding.act: <class 'torch.nn.modules.activation.SiLU'>
model.time_embedding.linear_2: <class 'qdiff.quant_layer.QuantModule'>
model.time_embedding.linear_2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.time_embedding.linear_2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.time_embedding.linear_2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.0: <class 'diffusers.models.unet_2d_blocks.CrossAttnDownBlock2D'>
model.down_blocks.0.attentions: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.0.attentions.0: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.down_blocks.0.attentions.0.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.0.attentions.0.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.0.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.0.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>

model.down_blocks.0.attentions.0.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>

model.down_blocks.0.attentions.0.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.0.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.0.attentions.0.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.0.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.0.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.0.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.1: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.down_blocks.0.attentions.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.0.attentions.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.0.attentions.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.down_blocks.0.attentions.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.0.attentions.1.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.attentions.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.attentions.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.attentions.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.resnets: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.0.resnets.0: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.down_blocks.0.resnets.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.resnets.0.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.0.resnets.0.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.resnets.0.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.0.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.0.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.resnets.0.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.resnets.0.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.0.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.0.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.resnets.0.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.0.resnets.0.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.0.resnets.0.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.resnets.0.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.0.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.0.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.resnets.1: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.down_blocks.0.resnets.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.resnets.1.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.0.resnets.1.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.resnets.1.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.1.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.1.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.resnets.1.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.resnets.1.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.1.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.1.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.resnets.1.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.0.resnets.1.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.0.resnets.1.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.resnets.1.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.1.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.resnets.1.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.0.downsamplers: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.0.downsamplers.0: <class 'diffusers.models.resnet.Downsample2D'>
model.down_blocks.0.downsamplers.0.conv: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.0.downsamplers.0.conv.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.downsamplers.0.conv.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.0.downsamplers.0.conv.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1: <class 'diffusers.models.unet_2d_blocks.CrossAttnDownBlock2D'>
model.down_blocks.1.attentions: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.1.attentions.0: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.down_blocks.1.attentions.0.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.1.attentions.0.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.0.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.0.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.1.attentions.0.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.down_blocks.1.attentions.0.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.0.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.1.attentions.0.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.0.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.0.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.0.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.1: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.down_blocks.1.attentions.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.1.attentions.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.1.attentions.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.down_blocks.1.attentions.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.1.attentions.1.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.attentions.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.attentions.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.attentions.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.resnets: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.1.resnets.0: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.down_blocks.1.resnets.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.resnets.0.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.1.resnets.0.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.resnets.0.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.0.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.0.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.resnets.0.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.resnets.0.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.0.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.0.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.resnets.0.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.1.resnets.0.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.1.resnets.0.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.resnets.0.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.0.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.0.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.resnets.0.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.resnets.0.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.0.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.0.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.resnets.1: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.down_blocks.1.resnets.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.resnets.1.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.1.resnets.1.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.resnets.1.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.1.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.1.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.resnets.1.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.resnets.1.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.1.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.1.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.resnets.1.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.1.resnets.1.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.1.resnets.1.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.resnets.1.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.1.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.resnets.1.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.1.downsamplers: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.1.downsamplers.0: <class 'diffusers.models.resnet.Downsample2D'>
model.down_blocks.1.downsamplers.0.conv: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.1.downsamplers.0.conv.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.downsamplers.0.conv.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.1.downsamplers.0.conv.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2: <class 'diffusers.models.unet_2d_blocks.CrossAttnDownBlock2D'>
model.down_blocks.2.attentions: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.2.attentions.0: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.down_blocks.2.attentions.0.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.2.attentions.0.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.0.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.0.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.2.attentions.0.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.down_blocks.2.attentions.0.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.0.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.2.attentions.0.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.0.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.0.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.0.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.1: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.down_blocks.2.attentions.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.2.attentions.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.2.attentions.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.down_blocks.2.attentions.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.down_blocks.2.attentions.1.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.attentions.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.attentions.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.attentions.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.resnets: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.2.resnets.0: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.down_blocks.2.resnets.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.resnets.0.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.2.resnets.0.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.resnets.0.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.0.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.0.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.resnets.0.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.resnets.0.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.0.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.0.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.resnets.0.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.2.resnets.0.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.2.resnets.0.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.resnets.0.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.0.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.0.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.resnets.0.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.resnets.0.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.0.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.0.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.resnets.1: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.down_blocks.2.resnets.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.resnets.1.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.2.resnets.1.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.resnets.1.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.1.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.1.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.resnets.1.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.resnets.1.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.1.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.1.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.resnets.1.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.2.resnets.1.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.2.resnets.1.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.resnets.1.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.1.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.resnets.1.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.2.downsamplers: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.2.downsamplers.0: <class 'diffusers.models.resnet.Downsample2D'>
model.down_blocks.2.downsamplers.0.conv: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.2.downsamplers.0.conv.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.downsamplers.0.conv.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.2.downsamplers.0.conv.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.3: <class 'diffusers.models.unet_2d_blocks.DownBlock2D'>
model.down_blocks.3.resnets: <class 'torch.nn.modules.container.ModuleList'>
model.down_blocks.3.resnets.0: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.down_blocks.3.resnets.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.3.resnets.0.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.3.resnets.0.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.3.resnets.0.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.0.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.0.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.3.resnets.0.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.3.resnets.0.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.0.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.0.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.3.resnets.0.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.3.resnets.0.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.3.resnets.0.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.3.resnets.0.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.0.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.0.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.3.resnets.1: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.down_blocks.3.resnets.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.3.resnets.1.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.3.resnets.1.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.3.resnets.1.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.1.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.1.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.3.resnets.1.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.3.resnets.1.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.1.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.1.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.down_blocks.3.resnets.1.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.down_blocks.3.resnets.1.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.down_blocks.3.resnets.1.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.down_blocks.3.resnets.1.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.1.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.down_blocks.3.resnets.1.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.0: <class 'diffusers.models.unet_2d_blocks.UpBlock2D'>
model.up_blocks.0.resnets: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.0.resnets.0: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.up_blocks.0.resnets.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.0.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.0.resnets.0.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.0.resnets.0.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.0.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.0.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.0.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.0.resnets.0.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.0.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.0.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.0.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.0.resnets.0.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.0.resnets.0.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.0.resnets.0.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.0.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.0.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.0.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.0.resnets.0.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.0.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.0.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.1: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.up_blocks.0.resnets.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.1.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.0.resnets.1.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.0.resnets.1.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.1.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.1.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.1.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.0.resnets.1.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.1.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.1.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.1.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.0.resnets.1.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.0.resnets.1.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.0.resnets.1.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.1.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.1.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.1.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.0.resnets.1.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.1.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.1.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.2: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.up_blocks.0.resnets.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.2.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.0.resnets.2.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.0.resnets.2.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.2.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.2.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.2.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.0.resnets.2.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.2.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.2.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.2.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.0.resnets.2.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.0.resnets.2.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.0.resnets.2.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.2.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.2.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.resnets.2.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.0.resnets.2.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.2.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.resnets.2.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.0.upsamplers: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.0.upsamplers.0: <class 'diffusers.models.resnet.Upsample2D'>
model.up_blocks.0.upsamplers.0.conv: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.0.upsamplers.0.conv.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.upsamplers.0.conv.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.0.upsamplers.0.conv.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1: <class 'diffusers.models.unet_2d_blocks.CrossAttnUpBlock2D'>
model.up_blocks.1.attentions: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.attentions.0: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.up_blocks.1.attentions.0.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.1.attentions.0.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.0.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.0.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.attentions.0.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.up_blocks.1.attentions.0.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.0.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.1.attentions.0.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.0.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.0.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.0.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.1: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.up_blocks.1.attentions.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.1.attentions.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.attentions.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.up_blocks.1.attentions.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.1.attentions.1.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.2: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.up_blocks.1.attentions.2.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.1.attentions.2.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.2.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.2.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.attentions.2.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.up_blocks.1.attentions.2.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.2.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.1.attentions.2.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.attentions.2.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.attentions.2.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.attentions.2.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.resnets.0: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.up_blocks.1.resnets.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.0.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.1.resnets.0.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.resnets.0.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.0.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.0.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.0.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.resnets.0.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.0.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.0.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.0.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.1.resnets.0.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.1.resnets.0.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.resnets.0.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.0.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.0.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.0.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.resnets.0.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.0.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.0.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.1: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.up_blocks.1.resnets.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.1.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.1.resnets.1.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.resnets.1.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.1.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.1.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.1.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.resnets.1.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.1.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.1.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.1.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.1.resnets.1.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.1.resnets.1.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.resnets.1.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.1.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.1.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.1.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.resnets.1.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.1.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.1.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.2: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.up_blocks.1.resnets.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.2.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.1.resnets.2.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.resnets.2.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.2.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.2.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.2.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.resnets.2.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.2.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.2.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.2.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.1.resnets.2.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.1.resnets.2.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.resnets.2.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.2.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.2.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.resnets.2.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.resnets.2.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.2.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.resnets.2.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.1.upsamplers: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.1.upsamplers.0: <class 'diffusers.models.resnet.Upsample2D'>
model.up_blocks.1.upsamplers.0.conv: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.1.upsamplers.0.conv.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.upsamplers.0.conv.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.1.upsamplers.0.conv.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2: <class 'diffusers.models.unet_2d_blocks.CrossAttnUpBlock2D'>
model.up_blocks.2.attentions: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.attentions.0: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.up_blocks.2.attentions.0.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.2.attentions.0.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.0.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.0.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.attentions.0.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.up_blocks.2.attentions.0.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.0.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.2.attentions.0.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.0.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.0.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.0.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.1: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.up_blocks.2.attentions.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.2.attentions.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.attentions.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.up_blocks.2.attentions.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.2.attentions.1.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.2: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.up_blocks.2.attentions.2.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.2.attentions.2.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.2.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.2.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.attentions.2.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.up_blocks.2.attentions.2.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.2.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.2.attentions.2.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.attentions.2.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.attentions.2.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.attentions.2.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.resnets.0: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.up_blocks.2.resnets.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.0.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.2.resnets.0.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.resnets.0.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.0.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.0.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.0.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.resnets.0.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.0.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.0.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.0.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.2.resnets.0.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.2.resnets.0.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.resnets.0.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.0.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.0.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.0.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.resnets.0.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.0.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.0.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.1: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.up_blocks.2.resnets.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.1.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.2.resnets.1.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.resnets.1.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.1.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.1.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.1.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.resnets.1.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.1.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.1.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.1.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.2.resnets.1.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.2.resnets.1.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.resnets.1.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.1.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.1.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.1.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.resnets.1.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.1.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.1.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.2: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.up_blocks.2.resnets.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.2.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.2.resnets.2.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.resnets.2.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.2.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.2.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.2.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.resnets.2.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.2.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.2.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.2.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.2.resnets.2.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.2.resnets.2.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.resnets.2.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.2.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.2.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.resnets.2.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.resnets.2.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.2.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.resnets.2.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.2.upsamplers: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.2.upsamplers.0: <class 'diffusers.models.resnet.Upsample2D'>
model.up_blocks.2.upsamplers.0.conv: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.2.upsamplers.0.conv.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.upsamplers.0.conv.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.2.upsamplers.0.conv.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3: <class 'diffusers.models.unet_2d_blocks.CrossAttnUpBlock2D'>
model.up_blocks.3.attentions: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.attentions.0: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.up_blocks.3.attentions.0.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.3.attentions.0.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.0.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.0.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.attentions.0.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.up_blocks.3.attentions.0.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.0.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.3.attentions.0.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.0.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.0.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.0.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.1: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.up_blocks.3.attentions.1.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.3.attentions.1.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.1.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.1.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.attentions.1.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.up_blocks.3.attentions.1.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.1.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.3.attentions.1.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.1.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.1.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.1.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.2: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.up_blocks.3.attentions.2.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.3.attentions.2.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.2.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.2.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.attentions.2.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.up_blocks.3.attentions.2.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.2.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.up_blocks.3.attentions.2.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.attentions.2.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.attentions.2.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.attentions.2.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets: <class 'torch.nn.modules.container.ModuleList'>
model.up_blocks.3.resnets.0: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.up_blocks.3.resnets.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.0.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.3.resnets.0.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.resnets.0.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.0.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.0.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.0.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.resnets.0.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.0.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.0.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.0.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.3.resnets.0.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.3.resnets.0.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.resnets.0.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.0.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.0.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.0.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.resnets.0.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.0.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.0.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.1: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.up_blocks.3.resnets.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.1.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.3.resnets.1.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.resnets.1.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.1.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.1.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.1.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.resnets.1.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.1.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.1.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.1.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.3.resnets.1.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.3.resnets.1.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.resnets.1.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.1.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.1.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.1.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.resnets.1.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.1.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.1.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.2: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.up_blocks.3.resnets.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.2.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.3.resnets.2.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.resnets.2.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.2.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.2.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.2.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.resnets.2.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.2.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.2.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.2.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.up_blocks.3.resnets.2.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.up_blocks.3.resnets.2.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.resnets.2.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.2.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.2.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.up_blocks.3.resnets.2.conv_shortcut: <class 'qdiff.quant_layer.QuantModule'>
model.up_blocks.3.resnets.2.conv_shortcut.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.2.conv_shortcut.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.up_blocks.3.resnets.2.conv_shortcut.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block: <class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'>
model.mid_block.attentions: <class 'torch.nn.modules.container.ModuleList'>
model.mid_block.attentions.0: <class 'diffusers.models.transformer_2d.Transformer2DModel'>
model.mid_block.attentions.0.norm: <class 'torch.nn.modules.normalization.GroupNorm'>
model.mid_block.attentions.0.proj_in: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.attentions.0.proj_in.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.proj_in.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.proj_in.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.attentions.0.transformer_blocks: <class 'torch.nn.modules.container.ModuleList'>
model.mid_block.attentions.0.transformer_blocks.0: <class 'qdiff.quant_block.QuantTransformerBlock'>
model.mid_block.attentions.0.transformer_blocks.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.attentions.0.transformer_blocks.0.norm1: <class 'torch.nn.modules.normalization.LayerNorm'>
model.mid_block.attentions.0.transformer_blocks.0.attn1: <class 'diffusers.models.attention_processor.Attention'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn1.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.norm2: <class 'torch.nn.modules.normalization.LayerNorm'>
model.mid_block.attentions.0.transformer_blocks.0.attn2: <class 'diffusers.models.attention_processor.Attention'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out: <class 'torch.nn.modules.container.ModuleList'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.1: <class 'torch.nn.modules.dropout.Dropout'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.act_quantizer_q: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.act_quantizer_k: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.act_quantizer_v: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.attn2.act_quantizer_w: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.norm3: <class 'torch.nn.modules.normalization.LayerNorm'>
model.mid_block.attentions.0.transformer_blocks.0.ff: <class 'diffusers.models.attention.FeedForward'>
model.mid_block.attentions.0.transformer_blocks.0.ff.net: <class 'torch.nn.modules.container.ModuleList'>
model.mid_block.attentions.0.transformer_blocks.0.ff.net.0: <class 'diffusers.models.activations.GEGLU'>
model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.attentions.0.transformer_blocks.0.ff.net.1: <class 'torch.nn.modules.dropout.Dropout'>
model.mid_block.attentions.0.transformer_blocks.0.ff.net.2: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.attentions.0.proj_out: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.attentions.0.proj_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.proj_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.attentions.0.proj_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.resnets: <class 'torch.nn.modules.container.ModuleList'>
model.mid_block.resnets.0: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.mid_block.resnets.0.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.0.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.resnets.0.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.mid_block.resnets.0.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.resnets.0.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.0.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.0.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.resnets.0.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.resnets.0.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.0.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.0.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.resnets.0.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.mid_block.resnets.0.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.mid_block.resnets.0.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.resnets.0.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.0.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.0.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.resnets.1: <class 'qdiff.quant_block.QuantResnetBlock2D'>
model.mid_block.resnets.1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.resnets.1.norm1: <class 'torch.nn.modules.normalization.GroupNorm'>
model.mid_block.resnets.1.conv1: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.resnets.1.conv1.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.1.conv1.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.1.conv1.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.resnets.1.time_emb_proj: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.resnets.1.time_emb_proj.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.1.time_emb_proj.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.1.time_emb_proj.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.mid_block.resnets.1.norm2: <class 'torch.nn.modules.normalization.GroupNorm'>
model.mid_block.resnets.1.dropout: <class 'torch.nn.modules.dropout.Dropout'>
model.mid_block.resnets.1.conv2: <class 'qdiff.quant_layer.QuantModule'>
model.mid_block.resnets.1.conv2.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.1.conv2.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.mid_block.resnets.1.conv2.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
model.conv_norm_out: <class 'torch.nn.modules.normalization.GroupNorm'>
model.conv_out: <class 'qdiff.quant_layer.QuantModule'>
model.conv_out.weight_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.conv_out.act_quantizer: <class 'qdiff.quant_layer.UniformAffineQuantizer'>
model.conv_out.activation_function: <class 'qdiff.quant_layer.StraightThrough'>
