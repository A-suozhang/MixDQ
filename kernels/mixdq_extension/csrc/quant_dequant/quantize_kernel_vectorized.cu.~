#include <ATen/cuda/CUDAContext.h>
#include <torch/extension.h>
#include <cuda_fp16.h>
#include <cuda_runtime.h>

#include "quantization_kernels.h"
#include <stdio.h>


// #define CAST_TO_FLOAT4(value) (reinterpret_cast<float4>(&(value))[0])
// #define CAST_TO_INT8_4(value) (reinterpret_cast<char4>(&(value))[0])

#define CAST_TO_FLOAT8(pointer)  (reinterpret_cast<float8 *>(&(pointer))[0])
#define CAST_TO_HALF8(pointer)   (reinterpret_cast<half8 *>(&(pointer))[0])
#define CAST_TO_INT8_8(pointer)  (reinterpret_cast<int8_8 *>(&(pointer))[0])

namespace mixdq {
namespace {

typedef struct int8 {
    int x1, x2, x3, x4, x5, x6, x7, x8;
} int8;

typedef struct float8 {
    float x1, x2, x3, x4, x5, x6, x7, x8;
} float8;

typedef struct int8_8 {
    int8_t x1, x2, x3, x4, x5, x6, x7, x8;
} int8_4;

typedef struct half8 {
    __half x1, x2, x3, x4, x5, x6, x7, x8;
} half8;

__global__ void quantize_to_int8_vectorized_kernel(int8_t *output, __half* input, const float* scale_inv_ptr, const float* zero_point_ptr, int64_t numel) {
    int tid = threadIdx.x;
    int bid = blockIdx.x;
    int global_id = bid * blockDim.x + tid;
    float scale_inv_val = *scale_inv_ptr;
    float zero_point_val = *zero_point_ptr;

    for (int i = global_id; i < numel; i += gridDim.x * blockDim.x * 8) {
        // Load 4 half elements into a float4
        half8 x_half = CAST_TO_HALF8(input[i]);
        float8 x;
        x.x1 = __half2float(x_half.x1);
        x.x2 = __half2float(x_half.x2);
        x.x3 = __half2float(x_half.x3);
        x.x4 = __half2float(x_half.x4);
        x.x5 = __half2float(x_half.x5);
        x.x6 = __half2float(x_half.x6);
        x.x7 = __half2float(x_half.x7);
        x.x8 = __half2float(x_half.x8);

        // Quantize each element
        int8 x_int;
        x_int.x1 = lrintf((x.x1 * scale_inv_val + zero_point_val));
        x_int.x2 = lrintf((x.x2 * scale_inv_val + zero_point_val));
        x_int.x3 = lrintf((x.x3 * scale_inv_val + zero_point_val));
        x_int.x4 = lrintf((x.x4 * scale_inv_val + zero_point_val));
        x_int.x5 = lrintf((x.x5 * scale_inv_val + zero_point_val));
        x_int.x6 = lrintf((x.x6 * scale_inv_val + zero_point_val));
        x_int.x7 = lrintf((x.x7 * scale_inv_val + zero_point_val));
        x_int.x8 = lrintf((x.x8 * scale_inv_val + zero_point_val));

        // Clamp to int8 range
        x_int.x1 = min(max(x_int.x1, -128), 127);
        x_int.x2 = min(max(x_int.x2, -128), 127);
        x_int.x3 = min(max(x_int.x3, -128), 127);
        x_int.x4 = min(max(x_int.x4, -128), 127);
        x_int.x5 = min(max(x_int.x5, -128), 127);
        x_int.x6 = min(max(x_int.x6, -128), 127);
        x_int.x7 = min(max(x_int.x7, -128), 127);
        x_int.x8 = min(max(x_int.x8, -128), 127);

        // Store as int8_t
        int8_8 x_int8;
        x_int8.x1 = static_cast<int8_t>(x_int.x1);
        x_int8.x2 = static_cast<int8_t>(x_int.x2);
        x_int8.x3 = static_cast<int8_t>(x_int.x3);
        x_int8.x4 = static_cast<int8_t>(x_int.x4);
        x_int8.x5 = static_cast<int8_t>(x_int.x5);
        x_int8.x6 = static_cast<int8_t>(x_int.x6);
        x_int8.x7 = static_cast<int8_t>(x_int.x7);
        x_int8.x8 = static_cast<int8_t>(x_int.x8);

        // Store the results
        CAST_TO_INT8_8(output[i]) = x_int8;
    }
}

}   // namespace {}


void quantize_to_int8_vectorized(at::Tensor input, 
                      const at::Tensor scale_inv,
                      const at::Tensor zero_point,
                      at::Tensor output)
{
    int64_t numel = input.numel();
    const int block_size = 256;  // original: 256
    int64_t grid_size = (numel/8 + block_size - 1) / block_size;
    cudaStream_t stream = at::cuda::getCurrentCUDAStream();
    quantize_to_int8_vectorized_kernel<<<grid_size, block_size, (size_t)0, stream>>>(
        reinterpret_cast<int8_t*>(output.data_ptr()),
        reinterpret_cast<__half*>(input.data_ptr()), 
        reinterpret_cast<float*>(scale_inv.data_ptr()),
        reinterpret_cast<float*>(zero_point.data_ptr()),
        numel
    );
}

} // namespace mixdq